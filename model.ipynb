{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from Loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# x_train = None\n",
    "# x_val = None\n",
    "x_train = DataLoader(BATCH_SIZE, 'training_generator')\n",
    "x_val = DataLoader(BATCH_SIZE, 'training_generator', val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossHistory = []\n",
    "valHistory = []\n",
    "\n",
    "class NnueCallbacks(tf.keras.callbacks.Callback):\n",
    "  def on_batch_end(self, batch, logs=None):\n",
    "    return\n",
    "    lossHistory[-1].append(logs['loss'])\n",
    "    if batch and batch % 15000 == 0:\n",
    "\n",
    "      pred = self.model.predict(x_val)\n",
    "      loss = (pred.flatten() - x_val.labels.flatten()[0:len(x_val)*BATCH_SIZE]) ** 2\n",
    "      loss = np.sum(loss) / len(loss)\n",
    "      valHistory[-1].append(loss)\n",
    "\n",
    "      for i in lossHistory:\n",
    "        plt.plot(i[20:], linewidth=0.5)\n",
    "        plt.savefig(\"loss.png\", dpi=400)\n",
    "      plt.clf()\n",
    "\n",
    "      for i in valHistory:\n",
    "        plt.plot(i, linewidth=0.5)\n",
    "        plt.savefig(\"val.png\", dpi=400)\n",
    "      \n",
    "      plt.clf()\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def clipped_relu(x):\n",
    "    return K.relu(x, max_value=4)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(769,), activation=clipped_relu))\n",
    "model.add(Dense(32, activation=clipped_relu))\n",
    "model.add(Dense(32, activation=clipped_relu))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"production/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "325481/325481 [==============================] - 539s 2ms/step - loss: 0.0177 - mae: 0.0843 - val_loss: 0.0167 - val_mae: 0.0799\n",
      "Epoch 2/100\n",
      "325481/325481 [==============================] - 540s 2ms/step - loss: 0.0164 - mae: 0.0796 - val_loss: 0.0164 - val_mae: 0.0792\n",
      "Epoch 3/100\n",
      "325481/325481 [==============================] - 542s 2ms/step - loss: 0.0162 - mae: 0.0788 - val_loss: 0.0163 - val_mae: 0.0780\n",
      "Epoch 4/100\n",
      "325481/325481 [==============================] - 541s 2ms/step - loss: 0.0161 - mae: 0.0784 - val_loss: 0.0163 - val_mae: 0.0779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56b4640df0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossHistory.append([])\n",
    "valHistory.append([])\n",
    "model.fit(x_train, validation_data=x_val, epochs=100, callbacks=[NnueCallbacks(), EarlyStopping(patience=1)])\n",
    "\n",
    "#Epoch 2/2\n",
    "#10906/10906 [==============================] - 225s 21ms/step - loss: 0.0243 - mae: 0.0866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15982483]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Features\n",
    "import numpy as np\n",
    "\n",
    "FEN = \"r3n1k1/2qbbppp/4p3/3p2P1/2p2P2/2N1P3/Pr1B2BP/R2Q1RK1 b - - 2 19\"\n",
    "features = Features.get(FEN)\n",
    "model.predict(np.array([features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 13:49:29.014897: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: production/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"production\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
